{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 904663\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import codecs\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "filename = './speeches.txt'\n",
    "\n",
    "file = unidecode.unidecode(codecs.open(filename, \"r\",encoding='utf-8', errors='ignore').read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ay? All of the ladies can come but the guys can't.\r\n",
      "But all of the people outside, we're going to take a bigger place because I feel slightly guilty.\r\n",
      "But look, we have a very serious mess on our hands\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 10\n",
      " 11\n",
      " 12\n",
      " 39\n",
      " 40\n",
      " 41\n",
      "[torch.LongTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor)\n",
    "\n",
    "print(char_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    pre=0\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        output_dist=np.exp(output_dist)\n",
    "        output_dist=output_dist/sum(output_dist)\n",
    "        pre+=np.log(output_dist[top_i])\n",
    "#         print (pre)\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "#     pre*=-1\n",
    "#     pre/=predict_len\n",
    "#     print(pre)\n",
    "    pre=np.exp(pre)\n",
    "    print(pre)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c])\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / chunk_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 18s (100 5%) 2.2349]\n",
      "nan\n",
      "Whes fohe fre haive gor, I has nedebemeas veevecat ire -- the andt be. Be \"omene.\r\n",
      "Tre yor going and t \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/s/sgarg15/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: overflow encountered in exp\n",
      "/home1/s/sgarg15/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 38s (200 10%) 1.9798]\n",
      "nan\n",
      "Wher what we hers. I the beang a the of ther comeny a I mansthing whey't you ow a know, o me wand buin \n",
      "\n",
      "[0m 57s (300 15%) 1.7887]\n",
      "nan\n",
      "Whe thake ushey've now of it's a very probly very what oup the breat ther to people ith 4 to proight i \n",
      "\n",
      "[1m 16s (400 20%) 2.4595]\n",
      "nan\n",
      "Whet razing thest have for -- wion tople.\n",
      "\n",
      "\n",
      "\n",
      "Tow thing tor thing they're get to fing tho wasghing  \n",
      "\n",
      "[1m 35s (500 25%) 1.9753]\n",
      "nan\n",
      "Wh, I was you stople a going the grica to get doing treblens - yreas a baid our have moake, tened we h \n",
      "\n",
      "[1m 55s (600 30%) 1.6869]\n",
      "nan\n",
      "Wh's Are by a got on grany in I winduted on endory werworw and that's baughting sany? You kord can't t \n",
      "\n",
      "[2m 14s (700 35%) 2.3102]\n",
      "nan\n",
      "Whing have in the lave billis. You know -- up frirs. We prortiing. Evers that he stas to him to bey. R \n",
      "\n",
      "[2m 33s (800 40%) 1.3451]\n",
      "nan\n",
      "Wh, I'm a lot in to longe bo the everybody. That welless. Ring it see our going to staties to sayn, ha \n",
      "\n",
      "[2m 52s (900 45%) 1.5886]\n",
      "nan\n",
      "Whike. Where then to know, you know of to 33%ning have said thereneshis And an the deals. They're goin \n",
      "\n",
      "[3m 12s (1000 50%) 1.5232]\n",
      "nan\n",
      "Whem. And you're ible don the nacbe, I wentratainsed they people. They don't woulient, you know, we're \n",
      "\n",
      "[3m 31s (1100 55%) 1.6639]\n",
      "nan\n",
      "Wher and I real hemst.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "But in LEOPLE BEBTAy, Carest from and the Und infred, what's berandrs \n",
      "\n",
      "[3m 50s (1200 60%) 1.5815]\n",
      "nan\n",
      "Whey take one the so mones it terlesss oflver in people.\n",
      "Estarth lory going to said. You know, it's n \n",
      "\n",
      "[4m 9s (1300 65%) 1.5271]\n",
      "nan\n",
      "Wher want Iraqs prost agaisp;, any somyif\n",
      "I mentist get I was so to people said, I'd be going to he a \n",
      "\n",
      "[4m 28s (1400 70%) 1.7310]\n",
      "nan\n",
      "Wht ince of thind, should have with his is had guys dover beliad ... We have agaigh stay, if better, s \n",
      "\n",
      "[4m 47s (1500 75%) 1.8472]\n",
      "nan\n",
      "Whald all the trader from and they we has did country. There'll get we have great - you know it of to  \n",
      "\n",
      "[5m 7s (1600 80%) 1.6405]\n",
      "nan\n",
      "What I said the know, the're come.\n",
      "\n",
      "\n",
      "Wen't have all terrar a billion, evespactaming as. We're going \n",
      "\n",
      "[5m 26s (1700 85%) 1.7052]\n",
      "nan\n",
      "Wh. We over like they'll me of tell, \"I rick yoes in a lotater we don't goou can the right? He happen. \n",
      "\n",
      "[5m 45s (1800 90%) 1.5637]\n",
      "nan\n",
      "Whes that we doed of -- I leave things Hismply going to bactos of My probabgice. These so me and the d \n",
      "\n",
      "[6m 5s (1900 95%) 1.7170]\n",
      "nan\n",
      "Whory and our the people. The stand and we're about I say finester. They deally three time are can the \n",
      "\n",
      "[6m 24s (2000 100%) 1.6718]\n",
      "nan\n",
      "Wh hea rusigread to a preting of the people lover like the everyll them to. We don't know suppeting ou \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "lr = 0.005\n",
    "\n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set())       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate('Th', 200000, temperature=0.8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate('Th', 200, temperature=0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate('Th', 200, temperature=1.4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate('Th', 9, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/s/sgarg15/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: overflow encountered in exp\n",
      "/home1/s/sgarg15/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "arks politer, believed people. I take's a ned a memuce they big could the ever mett the people be goint to be really than and they country. And that the here on then sit\r\n",
      "We have do somined up the negetion. We know think that everybody, want to be intheir lelleing. We're does, and an dispeting we're going to be didise's going to a lot hels, not, the very that's not going to the be them part -- in the evelope our cople. I have great chaign. We have Chrisued this dipan the creficit. We have the process.\"\r\n",
      "We're guy were all that are that cave the was deven that and we're great make of people of a so miling be tell have not want the deal.\r\n",
      "I was the didn't know hiple to get so bigger they want to leats we go the even she are the hereed. We're go, No's going to begning do. We know, I real, Pountrody - a disappens. And that ever by lave but the wone harn really is heat plause of the He great happend of thing.\" Wh endinas of the people, Clinton so does the compened people. They want even one seally we have the to like the negare so don't get the percisters paricians in we seathing.\r\n",
      "We all the vettal me tell super back a lot. We defent out no and we would neguple care an out they take the evens really he do every be get the people becaution even but they have compaine than them thip China, you're going to buse problem. We? We really deal have the problems. We can be a groter the and of their going to prend want this so the some in than the corver.  We're going so a workinfing the medoolle get didfed.\" We even hinut doing the prication because out the plantry I let one litthing. Not the be this in that when they gand on a grying problem. When have busines the mupler so money. I great state. I make need the have to conser. I lonks that is even this happen thet the mure and think. We're do. We're going to called and they doing two ther. We're going to do the than to is mes.\r\n",
      "We're never a people the what it. We're give the they doing, the compled as the medom ans everyatinald\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('a', 2000, temperature=0.8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
