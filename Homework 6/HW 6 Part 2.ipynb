{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "import codecs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = codecs.open(filename, \"r\",encoding='utf-8', errors='ignore').read().strip().split('\\n')\n",
    "    return [unicode_to_ascii(line) for line in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letter_to_tensor(letter,all_letters,n_letters):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    letter_index = all_letters.find(letter)\n",
    "    tensor[0][letter_index] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def line_to_tensor(line,all_letters,n_letters):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        letter_index = all_letters.find(letter)\n",
    "        tensor[li][0][letter_index] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_from_output(output,all_categories):\n",
    "    _, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    category_i = top_i[0][0]\n",
    "    return all_categories[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_pair(all_categories,category_lines,all_letters,n_letters):                                                                                                               \n",
    "    category = random.choice(all_categories)\n",
    "    line = random.choice(category_lines[category])\n",
    "    category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "    line_tensor = Variable(line_to_tensor(line,all_letters,n_letters))\n",
    "    return category, line, category_tensor, line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./train/af.txt', './train/de.txt', './train/fi.txt', './train/fr.txt', './train/in.txt', './train/ir.txt', './train/cn.txt', './train/za.txt', './train/pk.txt']\n",
      "['./val/af.txt', './val/cn.txt', './val/de.txt', './val/fi.txt', './val/fr.txt', './val/ir.txt', './val/za.txt', './val/pk.txt', './val/in.txt']\n"
     ]
    }
   ],
   "source": [
    "train_filenames = glob.glob('./train/*.txt')\n",
    "print(train_filenames)\n",
    "\n",
    "val_filenames = glob.glob('./val/*.txt')\n",
    "print (val_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "print(unicode_to_ascii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_categories = 9\n",
      "['tagegin', 'kheda', 'trichy', 'palaina', \"sainthilairede l'aude\"]\n"
     ]
    }
   ],
   "source": [
    "train_category_lines={}\n",
    "train_categories=[]\n",
    "for filename in train_filenames:\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    train_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    train_category_lines[category] = lines\n",
    "\n",
    "train_n_categories = len(train_categories)\n",
    "print('n_categories =', train_n_categories)\n",
    "print(train_category_lines['in'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_categories = 9\n",
      "['sinceira grande', 'rinangkatan', 'quinta das mares', 'saraguaina', 'dazhangjingqiao']\n"
     ]
    }
   ],
   "source": [
    "val_category_lines={}\n",
    "val_categories=[]\n",
    "for filename in val_filenames:\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    val_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    val_category_lines[category] = lines\n",
    "\n",
    "val_n_categories = len(val_categories)\n",
    "print('n_categories =', val_n_categories)\n",
    "print(val_category_lines['in'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_n_categories==val_n_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the category_lines dictionary, a list of names per language\n",
    "# category_lines = {}\n",
    "# all_categories = []\n",
    "\n",
    "\n",
    "\n",
    "# for filename in train_filenames:\n",
    "#     category = filename.split('/')[-1].split('.')[0]\n",
    "#     all_categories.append(category)\n",
    "#     lines = readLines(filename)\n",
    "#     category_lines[category] = lines\n",
    "\n",
    "# n_categories = len(all_categories)\n",
    "# print('n_categories =', n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 38 \n",
      "    0     0     0     0     0     0     0     0     0     1     0     0     0\n",
      "\n",
      "Columns 39 to 51 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 52 to 56 \n",
      "    0     0     0     0     0\n",
      "[torch.FloatTensor of size 1x57]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(letter_to_tensor('J',all_letters,n_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "print(line_to_tensor('Jones',all_letters,n_letters).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for Training\n",
    "\n",
    "Before going into training we should make a few helper functions. The first is to interpret the output of the network, which we know to be a likelihood of each category. We can use `Tensor.topk` to get the index of the greatest value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = cn / line = liuhuaping\n",
      "category = fi / line = jylhankyla\n",
      "category = in / line = linushka\n",
      "category = fi / line = fida ali bak\n",
      "category = pk / line = purano wah misri\n",
      "category = de / line = ziellechen\n",
      "category = ir / line = kotalah kamar\n",
      "category = pk / line = mufti kili\n",
      "category = za / line = zadra\n",
      "category = za / line = traktkurzan\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = random_training_pair(train_categories,train_category_lines,all_letters,n_letters)\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = fi / line = finsterbergen\n",
      "category = af / line = asolmah\n",
      "category = ir / line = kirchovon\n",
      "category = fr / line = xaffevillers\n",
      "category = de / line = dalena\n",
      "category = in / line = jingjiangao\n",
      "category = de / line = niederhost\n",
      "category = ir / line = towali bala\n",
      "category = ir / line = qaleh agha\n",
      "category = de / line = bohnsackerweide\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = random_training_pair(val_categories,val_category_lines,all_letters,n_letters)\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the [loss function `nn.NLLLoss`](http://pytorch.org/docs/nn.html#nllloss) RNN is `nn.LogSoftmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each loop of training will:\n",
    "\n",
    "* Create input and target tensors\n",
    "* Create a zeroed initial hidden state\n",
    "* Read each letter in and\n",
    "    * Keep hidden state for next letter\n",
    "* Compare final output to target\n",
    "* Back-propagate\n",
    "* Return the output and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, category_tensor, line_tensor):\n",
    "    rnn.zero_grad()\n",
    "    hidden = rnn.init_hidden()\n",
    "    \n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    \n",
    "    loss.backward() ##this computes the gradient i guess\n",
    "\n",
    "    optimizer.step() ##this updates the parameters after the gradient is computed\n",
    "\n",
    "    return output, loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, train_n_categories)\n",
    "hidden = rnn.init_hidden()\n",
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 2e-4 # If you set this too high, it might explode. If too low, it might not learn\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.size = torch.Size([1, 9])\n",
      "('za', 7)\n",
      "Variable containing:\n",
      "-2.2429 -2.1975 -2.1983 -2.2038 -2.1716 -2.1914 -2.1805 -2.1411 -2.2526\n",
      "[torch.FloatTensor of size 1x9]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/s/sgarg15/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "input = Variable(letter_to_tensor('A',all_letters,n_letters))\n",
    "\n",
    "output, next_hidden = rnn(input, hidden)\n",
    "print('output.size =', output.size())\n",
    "print(category_from_output(output,train_categories))\n",
    "\n",
    "input = Variable(line_to_tensor('Albert',all_letters,n_letters))\n",
    "hidden = Variable(torch.zeros(1, n_hidden))\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create an \"optimizer\" which updates the parameters of our model according to its gradients. We will use the vanilla SGD algorithm with a low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "accuracy=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/s/sgarg15/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5% (1m 7s) 2.0803 spilmai / fi ✗ (pk)\n",
      "10000 10% (2m 17s) 3.6756 popkeros / fr ✗ (pk)\n",
      "15000 15% (3m 25s) 1.9619 mbongweni / in ✗ (za)\n",
      "20000 20% (4m 25s) 0.1080 villeneuvetroloc / fr ✓\n",
      "25000 25% (5m 8s) 1.0132 arshabad / ir ✓\n",
      "30000 30% (6m 22s) 0.4874 qaryehye chashmehye aqa / af ✓\n",
      "35000 35% (7m 28s) 0.9143 miniera / in ✓\n",
      "40000 40% (8m 36s) 1.4253 aldeanueva de figueroa / fr ✗ (fi)\n",
      "45000 45% (9m 35s) 3.9009 kirianwali / pk ✗ (ir)\n",
      "50000 50% (10m 41s) 0.8852 xiala / cn ✓\n",
      "55000 55% (11m 40s) 1.1541 hejiawa / cn ✓\n",
      "60000 60% (12m 42s) 1.9910 pirzay caharbagh / ir ✗ (af)\n",
      "65000 65% (13m 50s) 3.0657 esperaza / za ✗ (fr)\n",
      "70000 70% (14m 46s) 1.1703 surkhkhula / fi ✗ (af)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-753fc71b1568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Get a random training input and target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_training_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_categories\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_category_lines\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_letters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_letters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguess_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategory_from_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_categories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-544675a8187d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rnn, category_tensor, line_tensor)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##this computes the gradient i guess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##this updates the parameters after the gradient is computed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Get a random training input and target\n",
    "    category, line, category_tensor, line_tensor = random_training_pair(train_categories,train_category_lines,all_letters,n_letters)\n",
    "    output, loss = train(rnn, category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "    guess, guess_i = category_from_output(output, train_categories)\n",
    "    \n",
    "    if guess==category:\n",
    "        accuracy+=1.\n",
    "    \n",
    "    # Print epoch number, loss, name and guess\n",
    "    if epoch % print_every == 0:\n",
    "        guess, guess_i = category_from_output(output, train_categories)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (epoch, epoch / n_epochs * 100, time_since(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy/=n_epochs\n",
    "accuracy*=100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Results\n",
    "\n",
    "Plotting the historical loss from `all_losses` shows the network learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Results\n",
    "\n",
    "To see how well the network performs on different categories, we will create a confusion matrix, indicating for every actual language (rows) which language the network guesses (columns). To calculate the confusion matrix a bunch of samples are run through the network with `evaluate()`, which is the same as `train()` minus the backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(val_n_categories, val_n_categories)\n",
    "n_confusion = 10000\n",
    "accuracy=0\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(rnn,line_tensor):\n",
    "    hidden = rnn.init_hidden()\n",
    "    \n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = random_training_pair(val_categories,val_category_lines,all_letters,n_letters)\n",
    "    output = evaluate(rnn,line_tensor)\n",
    "    guess, guess_i = category_from_output(output, val_categories)\n",
    "    if guess==category:\n",
    "        accuracy+=1\n",
    "    category_i = val_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "accuracy/=10000\n",
    "accuracy*=100\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Normalize by dividing every row by its sum\n",
    "for i in range(val_n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + val_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + val_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pick out bright spots off the main axis that show which languages it guesses incorrectly, e.g. Chinese for Korean, and Spanish for Italian. It seems to do very well with Greek, and very poorly with English (perhaps because of overlap with other languages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = readLines('./cities_test.txt')\n",
    "out_lines=[]\n",
    "for line in lines:\n",
    "    line_tensor = Variable(line_to_tensor(line,all_letters,n_letters))\n",
    "    output=evaluate(rnn,line_tensor)\n",
    "    guess, guess_i = category_from_output(output, val_categories)\n",
    "    out_lines.append(guess)\n",
    "\n",
    "with open ('labels.txt','w') as f:\n",
    "    for line in out_lines:\n",
    "        f.write(line+'\\n')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
